{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['intro.0.weight', 'intro.1.weight', 'intro.1.bias', 'intro.1.running_mean', 'intro.1.running_var', 'intro.1.num_batches_tracked', 'module_list.0.0.0.weight', 'module_list.0.0.1.weight', 'module_list.0.0.1.bias', 'module_list.0.0.1.running_mean', 'module_list.0.0.1.running_var', 'module_list.0.0.1.num_batches_tracked', 'module_list.0.1.0.weight', 'module_list.0.1.1.weight', 'module_list.0.1.1.bias', 'module_list.0.1.1.running_mean', 'module_list.0.1.1.running_var', 'module_list.0.1.1.num_batches_tracked', 'module_list.0.1.3.weight', 'module_list.0.1.4.weight', 'module_list.0.1.4.bias', 'module_list.0.1.4.running_mean', 'module_list.0.1.4.running_var', 'module_list.0.1.4.num_batches_tracked', 'module_list.1.0.0.weight', 'module_list.1.0.1.weight', 'module_list.1.0.1.bias', 'module_list.1.0.1.running_mean', 'module_list.1.0.1.running_var', 'module_list.1.0.1.num_batches_tracked', 'module_list.1.1.0.weight', 'module_list.1.1.1.weight', 'module_list.1.1.1.bias', 'module_list.1.1.1.running_mean', 'module_list.1.1.1.running_var', 'module_list.1.1.1.num_batches_tracked', 'module_list.1.1.3.weight', 'module_list.1.1.4.weight', 'module_list.1.1.4.bias', 'module_list.1.1.4.running_mean', 'module_list.1.1.4.running_var', 'module_list.1.1.4.num_batches_tracked', 'module_list.1.2.0.weight', 'module_list.1.2.1.weight', 'module_list.1.2.1.bias', 'module_list.1.2.1.running_mean', 'module_list.1.2.1.running_var', 'module_list.1.2.1.num_batches_tracked', 'module_list.1.2.3.weight', 'module_list.1.2.4.weight', 'module_list.1.2.4.bias', 'module_list.1.2.4.running_mean', 'module_list.1.2.4.running_var', 'module_list.1.2.4.num_batches_tracked', 'module_list.2.0.0.weight', 'module_list.2.0.1.weight', 'module_list.2.0.1.bias', 'module_list.2.0.1.running_mean', 'module_list.2.0.1.running_var', 'module_list.2.0.1.num_batches_tracked', 'module_list.2.1.0.weight', 'module_list.2.1.1.weight', 'module_list.2.1.1.bias', 'module_list.2.1.1.running_mean', 'module_list.2.1.1.running_var', 'module_list.2.1.1.num_batches_tracked', 'module_list.2.1.3.weight', 'module_list.2.1.4.weight', 'module_list.2.1.4.bias', 'module_list.2.1.4.running_mean', 'module_list.2.1.4.running_var', 'module_list.2.1.4.num_batches_tracked', 'module_list.2.2.0.weight', 'module_list.2.2.1.weight', 'module_list.2.2.1.bias', 'module_list.2.2.1.running_mean', 'module_list.2.2.1.running_var', 'module_list.2.2.1.num_batches_tracked', 'module_list.2.2.3.weight', 'module_list.2.2.4.weight', 'module_list.2.2.4.bias', 'module_list.2.2.4.running_mean', 'module_list.2.2.4.running_var', 'module_list.2.2.4.num_batches_tracked', 'module_list.2.3.0.weight', 'module_list.2.3.1.weight', 'module_list.2.3.1.bias', 'module_list.2.3.1.running_mean', 'module_list.2.3.1.running_var', 'module_list.2.3.1.num_batches_tracked', 'module_list.2.3.3.weight', 'module_list.2.3.4.weight', 'module_list.2.3.4.bias', 'module_list.2.3.4.running_mean', 'module_list.2.3.4.running_var', 'module_list.2.3.4.num_batches_tracked', 'module_list.2.4.0.weight', 'module_list.2.4.1.weight', 'module_list.2.4.1.bias', 'module_list.2.4.1.running_mean', 'module_list.2.4.1.running_var', 'module_list.2.4.1.num_batches_tracked', 'module_list.2.4.3.weight', 'module_list.2.4.4.weight', 'module_list.2.4.4.bias', 'module_list.2.4.4.running_mean', 'module_list.2.4.4.running_var', 'module_list.2.4.4.num_batches_tracked', 'module_list.2.5.0.weight', 'module_list.2.5.1.weight', 'module_list.2.5.1.bias', 'module_list.2.5.1.running_mean', 'module_list.2.5.1.running_var', 'module_list.2.5.1.num_batches_tracked', 'module_list.2.5.3.weight', 'module_list.2.5.4.weight', 'module_list.2.5.4.bias', 'module_list.2.5.4.running_mean', 'module_list.2.5.4.running_var', 'module_list.2.5.4.num_batches_tracked', 'module_list.2.6.0.weight', 'module_list.2.6.1.weight', 'module_list.2.6.1.bias', 'module_list.2.6.1.running_mean', 'module_list.2.6.1.running_var', 'module_list.2.6.1.num_batches_tracked', 'module_list.2.6.3.weight', 'module_list.2.6.4.weight', 'module_list.2.6.4.bias', 'module_list.2.6.4.running_mean', 'module_list.2.6.4.running_var', 'module_list.2.6.4.num_batches_tracked', 'module_list.2.7.0.weight', 'module_list.2.7.1.weight', 'module_list.2.7.1.bias', 'module_list.2.7.1.running_mean', 'module_list.2.7.1.running_var', 'module_list.2.7.1.num_batches_tracked', 'module_list.2.7.3.weight', 'module_list.2.7.4.weight', 'module_list.2.7.4.bias', 'module_list.2.7.4.running_mean', 'module_list.2.7.4.running_var', 'module_list.2.7.4.num_batches_tracked', 'module_list.2.8.0.weight', 'module_list.2.8.1.weight', 'module_list.2.8.1.bias', 'module_list.2.8.1.running_mean', 'module_list.2.8.1.running_var', 'module_list.2.8.1.num_batches_tracked', 'module_list.2.8.3.weight', 'module_list.2.8.4.weight', 'module_list.2.8.4.bias', 'module_list.2.8.4.running_mean', 'module_list.2.8.4.running_var', 'module_list.2.8.4.num_batches_tracked', 'module_list.3.0.0.weight', 'module_list.3.0.1.weight', 'module_list.3.0.1.bias', 'module_list.3.0.1.running_mean', 'module_list.3.0.1.running_var', 'module_list.3.0.1.num_batches_tracked', 'module_list.3.1.0.weight', 'module_list.3.1.1.weight', 'module_list.3.1.1.bias', 'module_list.3.1.1.running_mean', 'module_list.3.1.1.running_var', 'module_list.3.1.1.num_batches_tracked', 'module_list.3.1.3.weight', 'module_list.3.1.4.weight', 'module_list.3.1.4.bias', 'module_list.3.1.4.running_mean', 'module_list.3.1.4.running_var', 'module_list.3.1.4.num_batches_tracked', 'module_list.3.2.0.weight', 'module_list.3.2.1.weight', 'module_list.3.2.1.bias', 'module_list.3.2.1.running_mean', 'module_list.3.2.1.running_var', 'module_list.3.2.1.num_batches_tracked', 'module_list.3.2.3.weight', 'module_list.3.2.4.weight', 'module_list.3.2.4.bias', 'module_list.3.2.4.running_mean', 'module_list.3.2.4.running_var', 'module_list.3.2.4.num_batches_tracked', 'module_list.3.3.0.weight', 'module_list.3.3.1.weight', 'module_list.3.3.1.bias', 'module_list.3.3.1.running_mean', 'module_list.3.3.1.running_var', 'module_list.3.3.1.num_batches_tracked', 'module_list.3.3.3.weight', 'module_list.3.3.4.weight', 'module_list.3.3.4.bias', 'module_list.3.3.4.running_mean', 'module_list.3.3.4.running_var', 'module_list.3.3.4.num_batches_tracked', 'module_list.3.4.0.weight', 'module_list.3.4.1.weight', 'module_list.3.4.1.bias', 'module_list.3.4.1.running_mean', 'module_list.3.4.1.running_var', 'module_list.3.4.1.num_batches_tracked', 'module_list.3.4.3.weight', 'module_list.3.4.4.weight', 'module_list.3.4.4.bias', 'module_list.3.4.4.running_mean', 'module_list.3.4.4.running_var', 'module_list.3.4.4.num_batches_tracked', 'module_list.3.5.0.weight', 'module_list.3.5.1.weight', 'module_list.3.5.1.bias', 'module_list.3.5.1.running_mean', 'module_list.3.5.1.running_var', 'module_list.3.5.1.num_batches_tracked', 'module_list.3.5.3.weight', 'module_list.3.5.4.weight', 'module_list.3.5.4.bias', 'module_list.3.5.4.running_mean', 'module_list.3.5.4.running_var', 'module_list.3.5.4.num_batches_tracked', 'module_list.3.6.0.weight', 'module_list.3.6.1.weight', 'module_list.3.6.1.bias', 'module_list.3.6.1.running_mean', 'module_list.3.6.1.running_var', 'module_list.3.6.1.num_batches_tracked', 'module_list.3.6.3.weight', 'module_list.3.6.4.weight', 'module_list.3.6.4.bias', 'module_list.3.6.4.running_mean', 'module_list.3.6.4.running_var', 'module_list.3.6.4.num_batches_tracked', 'module_list.3.7.0.weight', 'module_list.3.7.1.weight', 'module_list.3.7.1.bias', 'module_list.3.7.1.running_mean', 'module_list.3.7.1.running_var', 'module_list.3.7.1.num_batches_tracked', 'module_list.3.7.3.weight', 'module_list.3.7.4.weight', 'module_list.3.7.4.bias', 'module_list.3.7.4.running_mean', 'module_list.3.7.4.running_var', 'module_list.3.7.4.num_batches_tracked', 'module_list.3.8.0.weight', 'module_list.3.8.1.weight', 'module_list.3.8.1.bias', 'module_list.3.8.1.running_mean', 'module_list.3.8.1.running_var', 'module_list.3.8.1.num_batches_tracked', 'module_list.3.8.3.weight', 'module_list.3.8.4.weight', 'module_list.3.8.4.bias', 'module_list.3.8.4.running_mean', 'module_list.3.8.4.running_var', 'module_list.3.8.4.num_batches_tracked', 'module_list.4.0.0.weight', 'module_list.4.0.1.weight', 'module_list.4.0.1.bias', 'module_list.4.0.1.running_mean', 'module_list.4.0.1.running_var', 'module_list.4.0.1.num_batches_tracked', 'module_list.4.1.0.weight', 'module_list.4.1.1.weight', 'module_list.4.1.1.bias', 'module_list.4.1.1.running_mean', 'module_list.4.1.1.running_var', 'module_list.4.1.1.num_batches_tracked', 'module_list.4.1.3.weight', 'module_list.4.1.4.weight', 'module_list.4.1.4.bias', 'module_list.4.1.4.running_mean', 'module_list.4.1.4.running_var', 'module_list.4.1.4.num_batches_tracked', 'module_list.4.2.0.weight', 'module_list.4.2.1.weight', 'module_list.4.2.1.bias', 'module_list.4.2.1.running_mean', 'module_list.4.2.1.running_var', 'module_list.4.2.1.num_batches_tracked', 'module_list.4.2.3.weight', 'module_list.4.2.4.weight', 'module_list.4.2.4.bias', 'module_list.4.2.4.running_mean', 'module_list.4.2.4.running_var', 'module_list.4.2.4.num_batches_tracked', 'module_list.4.3.0.weight', 'module_list.4.3.1.weight', 'module_list.4.3.1.bias', 'module_list.4.3.1.running_mean', 'module_list.4.3.1.running_var', 'module_list.4.3.1.num_batches_tracked', 'module_list.4.3.3.weight', 'module_list.4.3.4.weight', 'module_list.4.3.4.bias', 'module_list.4.3.4.running_mean', 'module_list.4.3.4.running_var', 'module_list.4.3.4.num_batches_tracked', 'module_list.4.4.0.weight', 'module_list.4.4.1.weight', 'module_list.4.4.1.bias', 'module_list.4.4.1.running_mean', 'module_list.4.4.1.running_var', 'module_list.4.4.1.num_batches_tracked', 'module_list.4.4.3.weight', 'module_list.4.4.4.weight', 'module_list.4.4.4.bias', 'module_list.4.4.4.running_mean', 'module_list.4.4.4.running_var', 'module_list.4.4.4.num_batches_tracked'])\n",
      "odict_keys(['harmonics.0.0.weight', 'harmonics.0.1.weight', 'harmonics.0.1.bias', 'harmonics.0.1.running_mean', 'harmonics.0.1.running_var', 'harmonics.0.1.num_batches_tracked', 'harmonics.0.3.weight', 'harmonics.0.4.weight', 'harmonics.0.4.bias', 'harmonics.0.4.running_mean', 'harmonics.0.4.running_var', 'harmonics.0.4.num_batches_tracked', 'harmonics.0.6.weight', 'harmonics.0.7.weight', 'harmonics.0.7.bias', 'harmonics.0.7.running_mean', 'harmonics.0.7.running_var', 'harmonics.0.7.num_batches_tracked', 'harmonics.0.9.weight', 'harmonics.0.10.weight', 'harmonics.0.10.bias', 'harmonics.0.10.running_mean', 'harmonics.0.10.running_var', 'harmonics.0.10.num_batches_tracked', 'harmonics.1.0.weight', 'harmonics.1.1.weight', 'harmonics.1.1.bias', 'harmonics.1.1.running_mean', 'harmonics.1.1.running_var', 'harmonics.1.1.num_batches_tracked', 'harmonics.1.3.weight', 'harmonics.1.4.weight', 'harmonics.1.4.bias', 'harmonics.1.4.running_mean', 'harmonics.1.4.running_var', 'harmonics.1.4.num_batches_tracked', 'harmonics.1.6.weight', 'harmonics.1.7.weight', 'harmonics.1.7.bias', 'harmonics.1.7.running_mean', 'harmonics.1.7.running_var', 'harmonics.1.7.num_batches_tracked', 'harmonics.1.9.weight', 'harmonics.1.10.weight', 'harmonics.1.10.bias', 'harmonics.1.10.running_mean', 'harmonics.1.10.running_var', 'harmonics.1.10.num_batches_tracked', 'harmonics.2.0.weight', 'harmonics.2.1.weight', 'harmonics.2.1.bias', 'harmonics.2.1.running_mean', 'harmonics.2.1.running_var', 'harmonics.2.1.num_batches_tracked', 'harmonics.2.3.weight', 'harmonics.2.4.weight', 'harmonics.2.4.bias', 'harmonics.2.4.running_mean', 'harmonics.2.4.running_var', 'harmonics.2.4.num_batches_tracked', 'harmonics.2.6.weight', 'harmonics.2.7.weight', 'harmonics.2.7.bias', 'harmonics.2.7.running_mean', 'harmonics.2.7.running_var', 'harmonics.2.7.num_batches_tracked', 'harmonics.2.9.weight', 'harmonics.2.10.weight', 'harmonics.2.10.bias', 'harmonics.2.10.running_mean', 'harmonics.2.10.running_var', 'harmonics.2.10.num_batches_tracked', 'splitted_harmonic.0.0.0.weight', 'splitted_harmonic.0.0.1.weight', 'splitted_harmonic.0.0.1.bias', 'splitted_harmonic.0.0.1.running_mean', 'splitted_harmonic.0.0.1.running_var', 'splitted_harmonic.0.0.1.num_batches_tracked', 'splitted_harmonic.0.1.0.weight', 'splitted_harmonic.0.1.1.weight', 'splitted_harmonic.0.1.1.bias', 'splitted_harmonic.0.1.1.running_mean', 'splitted_harmonic.0.1.1.running_var', 'splitted_harmonic.0.1.1.num_batches_tracked', 'splitted_harmonic.1.0.0.weight', 'splitted_harmonic.1.0.1.weight', 'splitted_harmonic.1.0.1.bias', 'splitted_harmonic.1.0.1.running_mean', 'splitted_harmonic.1.0.1.running_var', 'splitted_harmonic.1.0.1.num_batches_tracked', 'splitted_harmonic.1.1.0.weight', 'splitted_harmonic.1.1.1.weight', 'splitted_harmonic.1.1.1.bias', 'splitted_harmonic.1.1.1.running_mean', 'splitted_harmonic.1.1.1.running_var', 'splitted_harmonic.1.1.1.num_batches_tracked', 'splitted_harmonic.2.0.0.weight', 'splitted_harmonic.2.0.1.weight', 'splitted_harmonic.2.0.1.bias', 'splitted_harmonic.2.0.1.running_mean', 'splitted_harmonic.2.0.1.running_var', 'splitted_harmonic.2.0.1.num_batches_tracked', 'splitted_harmonic.2.1.0.weight', 'splitted_harmonic.2.1.1.weight', 'splitted_harmonic.2.1.1.bias', 'splitted_harmonic.2.1.1.running_mean', 'splitted_harmonic.2.1.1.running_var', 'splitted_harmonic.2.1.1.num_batches_tracked', 'preludes.0.weight', 'preludes.0.bias', 'preludes.1.weight', 'preludes.1.bias', 'preludes.2.weight', 'preludes.2.bias', 'equalizers_for_routes.0.0.weight', 'equalizers_for_routes.0.1.weight', 'equalizers_for_routes.0.1.bias', 'equalizers_for_routes.0.1.running_mean', 'equalizers_for_routes.0.1.running_var', 'equalizers_for_routes.0.1.num_batches_tracked', 'equalizers_for_routes.1.0.weight', 'equalizers_for_routes.1.1.weight', 'equalizers_for_routes.1.1.bias', 'equalizers_for_routes.1.1.running_mean', 'equalizers_for_routes.1.1.running_var', 'equalizers_for_routes.1.1.num_batches_tracked'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import cv2\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor, cat, sigmoid, exp, stack, max, meshgrid, linspace, arange\n",
    "from model import Darknet, Tail, Head\n",
    "\n",
    "\n",
    "from weight_formater import darknet, tail, weight, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5058084"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61576342"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"/home/ivan/Desktop/BN-UQ280_FEDUCI_GR_20170810142213.jpg\")\n",
    "image = cv2.resize(image, (416, 416), interpolation=cv2.INTER_LINEAR)\n",
    "image_tensor = image/255.0     \n",
    "image_tensor = Variable(torch.from_numpy(image_tensor).float())\n",
    "image_tensor = image_tensor.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = Head(Tensor([[120, 93],  [84,191], [238,186]]),1)\n",
    "anchors=Tensor([[120, 93],  [84,191], [238,186]]).view(3, 1, 1, 2)\n",
    "head_1 = Head(Tensor([[22, 59],  [61, 43],  [50, 97]]),1)\n",
    "anchors_1=Tensor([[22, 59],  [61, 43],  [50, 97]]).view(3, 1, 1, 2)\n",
    "head_2 = Head(Tensor([[12, 25],  [23, 16],  [36, 26]]),1)\n",
    "anchors_2 = Tensor([[12, 25],  [23, 16],  [36, 26]]).view(3, 1, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark = tail(darknet(image_tensor.unsqueeze(dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dark[0]\n",
    "features_1 = dark[1]\n",
    "features_2 = dark[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = list(features.size()[-2:])\n",
    "cells_offsets = stack(meshgrid(((arange(0, grid_size[0]))/ 13.),\n",
    "                               ((arange(0, grid_size[0]))/ 13.)), -1)\n",
    "\n",
    "grid_size_1 = list(features_1.size()[-2:])\n",
    "cells_offsets_1 = stack(meshgrid(((arange(0, grid_size_1[0]))/ 26.),\n",
    "                               ((arange(0, grid_size_1[0]))/ 26.)), -1)\n",
    "\n",
    "grid_size_2 = list(features_2.size()[-2:])\n",
    "cells_offsets_2 = stack(meshgrid(((arange(0, grid_size_2[0]))/ 52.),\n",
    "                               ((arange(0, grid_size_2[0]))/ 52.)), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.view([-1, len(anchors),number_of_classes + 5] + grid_size) \\\n",
    "        .permute(0, 1, 3, 4, 2) \\\n",
    "        .contiguous()\n",
    "features_1 = features_1.view([-1, len(anchors), number_of_classes + 5] + grid_size_1) \\\n",
    "        .permute(0, 1, 3, 4, 2) \\\n",
    "        .contiguous()\n",
    "features_2 = features_2.view([-1, len(anchors), number_of_classes + 5] + grid_size_2) \\\n",
    "        .permute(0, 1, 3, 4, 2) \\\n",
    "        .contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = sigmoid(features[..., :2]) / Tensor(grid_size) + cells_offsets\n",
    "sizes = exp(features[..., 2:4]) * anchors\n",
    "probabilities = sigmoid(features[..., 4:])\n",
    "\n",
    "centers_1 = sigmoid(features_1[..., :2]) / Tensor(grid_size_1) + cells_offsets_1\n",
    "sizes_1 = exp(features_1[..., 2:4]) * anchors_1\n",
    "probabilities_1 = sigmoid(features_1[..., 4:])\n",
    "\n",
    "centers_2 = sigmoid(features_2[..., :2]) / Tensor(grid_size_2) + cells_offsets_2\n",
    "sizes_2 = exp(features_2[..., 2:4]) * anchors_2\n",
    "probabilities_2 = sigmoid(features_2[..., 4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = pd.DataFrame(centers.view(-1,2).detach().numpy() * 416, columns=[\"center_x\",\"center_y\"])\n",
    "boxes[\"width\"]=0\n",
    "boxes['height']=0\n",
    "boxes[\"obj\"] = probabilities.view(-1,2)[...,0].detach().numpy()\n",
    "boxes[[\"width\",\"height\"]] = sizes.view(-1,2).detach().numpy()\n",
    "boxes[\"obj\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_1 = pd.DataFrame(centers_1.view(-1,2).detach().numpy() * 416, columns=[\"center_x\",\"center_y\"])\n",
    "boxes_1[\"width\"]=0\n",
    "boxes_1['height']=0\n",
    "boxes_1[\"obj\"] = probabilities_1.view(-1,2)[...,0].detach().numpy()\n",
    "boxes_1[[\"width\",\"height\"]] = sizes_1.view(-1,2).detach().numpy()\n",
    "boxes_1[\"obj\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_2 = pd.DataFrame(centers_2.view(-1,2).detach().numpy() * 416, columns=[\"center_x\",\"center_y\"])\n",
    "boxes_2[\"width\"]=0\n",
    "boxes_2['height']=0\n",
    "boxes_2[\"obj\"] = probabilities_2.view(-1,2)[...,0].detach().numpy()\n",
    "boxes_2[[\"width\",\"height\"]] = sizes_2.view(-1,2).detach().numpy()\n",
    "boxes_2[\"obj\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes =boxes.append(boxes_1).append(boxes_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.boxes_manipulations import convert_to_matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for x,y, w, h in boxes.loc[boxes[\"obj\"]>0.1,[\"center_x\", \"center_y\",\"width\", \"height\"]].values:\n",
    "       ax.add_patch(plt.Rectangle((int(x+w/2), int(y+h/2)), int(w), int(h), linewidth=4, fill=False))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(self, weightfile):\n",
    "        \n",
    "        #Open the weights file\n",
    "        fp = open(weightfile, \"rb\")\n",
    "\n",
    "        #The first 4 values are header information \n",
    "        # 1. Major version number\n",
    "        # 2. Minor Version Number\n",
    "        # 3. Subversion number \n",
    "        # 4. IMages seen \n",
    "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "        self.header = torch.from_numpy(header)\n",
    "        self.seen = self.header[3]\n",
    "        \n",
    "        #The rest of the values are the weights\n",
    "        # Let's load them up\n",
    "        weights = np.fromfile(fp, dtype = np.float32)\n",
    "        \n",
    "        ptr = 0\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type = self.blocks[i + 1][\"type\"]\n",
    "            \n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "                \n",
    "                conv = model[0]\n",
    "                \n",
    "                if (batch_normalize):\n",
    "                    bn = model[1]\n",
    "                    \n",
    "                    #Get the number of weights of Batch Norm Layer\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "                    \n",
    "                    #Load the weights\n",
    "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "                    \n",
    "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "                    \n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "                    \n",
    "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "                    \n",
    "                    #Cast the loaded weights into dims of model weights. \n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
    "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "\n",
    "                    #Copy the data to model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "                \n",
    "                else:\n",
    "                    #Number of biases\n",
    "                    num_biases = conv.bias.numel()\n",
    "                \n",
    "                    #Load the weights\n",
    "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
    "                    ptr = ptr + num_biases\n",
    "                    \n",
    "                    #reshape the loaded weights according to the dims of the model weights\n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                    \n",
    "                    #Finally copy the data\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "                    \n",
    "                    \n",
    "                #Let us load the weights for the Convolutional layers\n",
    "                num_weights = conv.weight.numel()\n",
    "                \n",
    "                #Do the same as above for weights\n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "                ptr = ptr + num_weights\n",
    "\n",
    "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "                conv.weight.data.copy_(conv_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
