{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import cv2\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor, cat, sigmoid, exp, stack, max, meshgrid, linspace, arange\n",
    "from model import Darknet, Tail, Head, deTail\n",
    "\n",
    "from torch.nn import Conv2d\n",
    "#from weight_formater import darknet, tail, we#ight, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Conv2d(3,4,5, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail = deTail(1, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "for module in tail.tails.children():\n",
    "    for modul in module.children():\n",
    "        for modu in modul.children():\n",
    "            if \"bias\" in str(modu):\n",
    "                print(modu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7bdfb08c1c1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for name, param in tail.tails.modules():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deTail(\n",
      "  (tails): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "        (3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1)\n",
      "        (6): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): LeakyReLU(negative_slope=0.1)\n",
      "        (9): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (11): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(1024, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "        (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1)\n",
      "        (6): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): LeakyReLU(negative_slope=0.1)\n",
      "        (9): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (11): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "        (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1)\n",
      "        (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): LeakyReLU(negative_slope=0.1)\n",
      "        (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (11): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(256, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for module in tail.modules():\n",
    "    for modu in module.modules():\n",
    "        print(modu)\n",
    "        break\n",
    "    #print(module)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tails.0.0.0.weight\n",
      "tails.0.0.1.weight\n",
      "tails.0.0.1.bias\n",
      "tails.0.0.1.running_mean\n",
      "tails.0.0.1.running_var\n",
      "tails.0.0.1.num_batches_tracked\n",
      "tails.0.0.3.weight\n",
      "tails.0.0.4.weight\n",
      "tails.0.0.4.bias\n",
      "tails.0.0.4.running_mean\n",
      "tails.0.0.4.running_var\n",
      "tails.0.0.4.num_batches_tracked\n",
      "tails.0.0.6.weight\n",
      "tails.0.0.7.weight\n",
      "tails.0.0.7.bias\n",
      "tails.0.0.7.running_mean\n",
      "tails.0.0.7.running_var\n",
      "tails.0.0.7.num_batches_tracked\n",
      "tails.0.0.9.weight\n",
      "tails.0.0.10.weight\n",
      "tails.0.0.10.bias\n",
      "tails.0.0.10.running_mean\n",
      "tails.0.0.10.running_var\n",
      "tails.0.0.10.num_batches_tracked\n",
      "tails.0.1.0.weight\n",
      "tails.0.1.1.weight\n",
      "tails.0.1.1.bias\n",
      "tails.0.1.1.running_mean\n",
      "tails.0.1.1.running_var\n",
      "tails.0.1.1.num_batches_tracked\n",
      "tails.0.2.0.weight\n",
      "tails.0.2.1.weight\n",
      "tails.0.2.1.bias\n",
      "tails.0.2.1.running_mean\n",
      "tails.0.2.1.running_var\n",
      "tails.0.2.1.num_batches_tracked\n",
      "tails.0.3.0.weight\n",
      "tails.0.3.0.bias\n",
      "tails.0.4.0.weight\n",
      "tails.0.4.1.weight\n",
      "tails.0.4.1.bias\n",
      "tails.0.4.1.running_mean\n",
      "tails.0.4.1.running_var\n",
      "tails.0.4.1.num_batches_tracked\n",
      "tails.1.0.0.weight\n",
      "tails.1.0.1.weight\n",
      "tails.1.0.1.bias\n",
      "tails.1.0.1.running_mean\n",
      "tails.1.0.1.running_var\n",
      "tails.1.0.1.num_batches_tracked\n",
      "tails.1.0.3.weight\n",
      "tails.1.0.4.weight\n",
      "tails.1.0.4.bias\n",
      "tails.1.0.4.running_mean\n",
      "tails.1.0.4.running_var\n",
      "tails.1.0.4.num_batches_tracked\n",
      "tails.1.0.6.weight\n",
      "tails.1.0.7.weight\n",
      "tails.1.0.7.bias\n",
      "tails.1.0.7.running_mean\n",
      "tails.1.0.7.running_var\n",
      "tails.1.0.7.num_batches_tracked\n",
      "tails.1.0.9.weight\n",
      "tails.1.0.10.weight\n",
      "tails.1.0.10.bias\n",
      "tails.1.0.10.running_mean\n",
      "tails.1.0.10.running_var\n",
      "tails.1.0.10.num_batches_tracked\n",
      "tails.1.1.0.weight\n",
      "tails.1.1.1.weight\n",
      "tails.1.1.1.bias\n",
      "tails.1.1.1.running_mean\n",
      "tails.1.1.1.running_var\n",
      "tails.1.1.1.num_batches_tracked\n",
      "tails.1.2.0.weight\n",
      "tails.1.2.1.weight\n",
      "tails.1.2.1.bias\n",
      "tails.1.2.1.running_mean\n",
      "tails.1.2.1.running_var\n",
      "tails.1.2.1.num_batches_tracked\n",
      "tails.1.3.0.weight\n",
      "tails.1.3.0.bias\n",
      "tails.1.4.0.weight\n",
      "tails.1.4.1.weight\n",
      "tails.1.4.1.bias\n",
      "tails.1.4.1.running_mean\n",
      "tails.1.4.1.running_var\n",
      "tails.1.4.1.num_batches_tracked\n",
      "tails.2.0.0.weight\n",
      "tails.2.0.1.weight\n",
      "tails.2.0.1.bias\n",
      "tails.2.0.1.running_mean\n",
      "tails.2.0.1.running_var\n",
      "tails.2.0.1.num_batches_tracked\n",
      "tails.2.0.3.weight\n",
      "tails.2.0.4.weight\n",
      "tails.2.0.4.bias\n",
      "tails.2.0.4.running_mean\n",
      "tails.2.0.4.running_var\n",
      "tails.2.0.4.num_batches_tracked\n",
      "tails.2.0.6.weight\n",
      "tails.2.0.7.weight\n",
      "tails.2.0.7.bias\n",
      "tails.2.0.7.running_mean\n",
      "tails.2.0.7.running_var\n",
      "tails.2.0.7.num_batches_tracked\n",
      "tails.2.0.9.weight\n",
      "tails.2.0.10.weight\n",
      "tails.2.0.10.bias\n",
      "tails.2.0.10.running_mean\n",
      "tails.2.0.10.running_var\n",
      "tails.2.0.10.num_batches_tracked\n",
      "tails.2.1.0.weight\n",
      "tails.2.1.1.weight\n",
      "tails.2.1.1.bias\n",
      "tails.2.1.1.running_mean\n",
      "tails.2.1.1.running_var\n",
      "tails.2.1.1.num_batches_tracked\n",
      "tails.2.2.0.weight\n",
      "tails.2.2.1.weight\n",
      "tails.2.2.1.bias\n",
      "tails.2.2.1.running_mean\n",
      "tails.2.2.1.running_var\n",
      "tails.2.2.1.num_batches_tracked\n",
      "tails.2.3.0.weight\n",
      "tails.2.3.0.bias\n"
     ]
    }
   ],
   "source": [
    "for key in tail.state_dict():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknet.eval()\n",
    "tail.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"/home/ivan/Desktop/BN-UQ280_FEDUCI_GR_20170810142213.jpg\")\n",
    "image = cv2.resize(image, (416, 416), interpolation=cv2.INTER_LINEAR)\n",
    "img_ =  image[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "img_ = img_[np.newaxis,:,:,:]/255.0  \n",
    "image_tensor = img_     \n",
    "image_tensor = Variable(torch.from_numpy(image_tensor).float())\n",
    "#image_tensor = image_tensor.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = Head(Tensor([[120, 93],  [84,191], [238,186]]),1)\n",
    "anchors=Tensor([[120, 93],  [84,191], [238,186]]).view(3, 1, 1, 2)\n",
    "head_1 = Head(Tensor([[22, 59],  [61, 43],  [50, 97]]),1)\n",
    "anchors_1=Tensor([[22, 59],  [61, 43],  [50, 97]]).view(3, 1, 1, 2)\n",
    "head_2 = Head(Tensor([[12, 25],  [23, 16],  [36, 26]]),1)\n",
    "anchors_2 = Tensor([[12, 25],  [23, 16],  [36, 26]]).view(3, 1, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark = tail(darknet(image_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dark[0]\n",
    "features_1 = dark[1]\n",
    "features_2 = dark[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = list(features.size()[-2:])\n",
    "cells_offsets = stack(meshgrid(((arange(0, grid_size[0]))/ 13.),\n",
    "                               ((arange(0, grid_size[0]))/ 13.)), -1)[...,[1,0]]\n",
    "\n",
    "grid_size_1 = list(features_1.size()[-2:])\n",
    "cells_offsets_1 = stack(meshgrid(((arange(0, grid_size_1[0]))/ 26.),\n",
    "                               ((arange(0, grid_size_1[0]))/ 26.)), -1)[...,[1,0]]\n",
    "\n",
    "grid_size_2 = list(features_2.size()[-2:])\n",
    "cells_offsets_2 = stack(meshgrid(((arange(0, grid_size_2[0]))/ 52.),\n",
    "                               ((arange(0, grid_size_2[0]))/ 52.)), -1)[...,[1,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.view([-1, len(anchors),number_of_classes + 5] + grid_size) \\\n",
    "        .permute(0, 1, 3, 4, 2) \\\n",
    "        .contiguous()\n",
    "features_1 = features_1.view([-1, len(anchors), number_of_classes + 5] + grid_size_1) \\\n",
    "        .permute(0, 1, 3, 4, 2) \\\n",
    "        .contiguous()\n",
    "features_2 = features_2.view([-1, len(anchors), number_of_classes + 5] + grid_size_2) \\\n",
    "        .permute(0, 1, 3, 4, 2) \\\n",
    "        .contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = sigmoid(features[..., :2]) / Tensor(grid_size) + cells_offsets\n",
    "sizes = exp(features[..., 2:4]) * anchors\n",
    "probabilities = sigmoid(features[..., 4:])\n",
    "\n",
    "centers_1 = sigmoid(features_1[..., :2]) / Tensor(grid_size_1) + cells_offsets_1\n",
    "sizes_1 = exp(features_1[..., 2:4]) * anchors_1\n",
    "probabilities_1 = sigmoid(features_1[..., 4:])\n",
    "\n",
    "centers_2 = sigmoid(features_2[..., :2]) / Tensor(grid_size_2) + cells_offsets_2\n",
    "sizes_2 = exp(features_2[..., 2:4]) * anchors_2\n",
    "probabilities_2 = sigmoid(features_2[..., 4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, sizes, probabilities = head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_1, sizes_1, probabilities_1 = head_1(features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_2, sizes_2, probabilities_2 = head_2(features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = pd.DataFrame(centers.view(-1,2).detach().numpy() * 416, columns=[\"center_x\",\"center_y\"])\n",
    "boxes[\"width\"]=0\n",
    "boxes['height']=0\n",
    "boxes[\"obj\"] = probabilities.view(-1,2)[...,0].detach().numpy()\n",
    "boxes[[\"width\",\"height\"]] = sizes.view(-1,2).detach().numpy()\n",
    "boxes[\"obj\"].max()\n",
    "\n",
    "boxes_1 = pd.DataFrame(centers_1.view(-1,2).detach().numpy() * 416, columns=[\"center_x\",\"center_y\"])\n",
    "boxes_1[\"width\"]=0\n",
    "boxes_1['height']=0\n",
    "boxes_1[\"obj\"] = probabilities_1.view(-1,2)[...,0].detach().numpy()\n",
    "boxes_1[[\"width\",\"height\"]] = sizes_1.view(-1,2).detach().numpy()\n",
    "boxes_1[\"obj\"].max()\n",
    "\n",
    "boxes_2 = pd.DataFrame(centers_2.view(-1,2).detach().numpy() * 416, columns=[\"center_x\",\"center_y\"])\n",
    "boxes_2[\"width\"]=0\n",
    "boxes_2['height']=0\n",
    "boxes_2[\"obj\"] = probabilities_2.view(-1,2)[...,0].detach().numpy()\n",
    "boxes_2[[\"width\",\"height\"]] = sizes_2.view(-1,2).detach().numpy()\n",
    "boxes_2[\"obj\"].max()\n",
    "boxes =boxes.append(boxes_1).append(boxes_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.boxes_manipulations import convert_to_matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for x,y, w, h in boxes.loc[boxes[\"obj\"]>0.1,[\"center_x\", \"center_y\",\"width\", \"height\"]].values:\n",
    "       #ax.plot([x],[y],\"r+\", linewidth=9)\n",
    "       ax.add_patch(plt.Rectangle((int(x-w/2), int(y-h/2)), int(w), int(h), linewidth=4, fill=False))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = list(features_1.size()[-2:])\n",
    "cells_offsets = stack(meshgrid(linspace(0, 1 - 1 / grid_size[0], grid_size[0]),\n",
    "                                       linspace(0, 1 - 1 / grid_size[1], grid_size[1])), -1)[...,[1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.arange(25)\n",
    "a,b = np.meshgrid(grid, grid)\n",
    "\n",
    "x_offset = torch.FloatTensor(a).view(-1,1)\n",
    "y_offset = torch.FloatTensor(b).view(-1,1)\n",
    "torch.cat((x_offset, y_offset), 1).unsqueeze(0)/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
